import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForTokenClassification, AdamW
from seqeval.metrics import classification_report, f1_score
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')

# 设置随机种子
SEED = 42
torch.manual_seed(SEED)
np.random.seed(SEED)

LABEL_LIST = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']
LABEL2ID = {label: idx for idx, label in enumerate(LABEL_LIST)}
ID2LABEL = {idx: label for idx, label in enumerate(LABEL_LIST)}


class NERDataset(Dataset):
    """NER数据集类"""

    def __init__(self, sentences, labels, tokenizer, max_len=128):
        self.sentences = sentences
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        sentence = self.sentences[idx]
        labels = self.labels[idx]

        # 对句子和标签进行编码
        encoding = self.tokenizer(
            sentence,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
            is_split_into_words=True
        )

        # 对齐标签（处理BERT的subword tokenization）
        aligned_labels = []
        word_ids = encoding.word_ids()
        previous_word_idx = None

        for word_idx in word_ids:
            if word_idx is None:
                # 特殊token：[CLS], [SEP], [PAD]
                aligned_labels.append(-100)
            elif word_idx != previous_word_idx:
                # 当前单词的第一个subword
                aligned_labels.append(LABEL2ID[labels[word_idx]])
            else:
                # 当前单词的后续subwords
                aligned_labels.append(LABEL2ID[labels[word_idx]] if labels[word_idx].startswith('I') else -100)
            previous_word_idx = word_idx

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(aligned_labels, dtype=torch.long)
        }


def create_sample_data():
    """创建示例训练数据"""
    sentences = [
        ['北京', '是', '中国', '的', '首都'],
        ['李华', '在', '清华大学', '学习', '计算机', '科学'],
        ['苹果', '公司', '发布', '了', '新', '款', 'iPhone'],
        ['上海', '位于', '中国', '东部']
    ]

    labels = [
        ['B-LOC', 'O', 'B-LOC', 'O', 'O'],
        ['B-PER', 'O', 'B-ORG', 'O', 'O', 'O'],
        ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'B-MISC'],
        ['B-LOC', 'O', 'B-LOC', 'O']
    ]

    return sentences, labels

class BERT_NER_Trainer:
    """BERT NER训练器"""

    def __init__(self, model_name='bert-base-chinese', num_labels=len(LABEL_LIST)):
        self.tokenizer = BertTokenizer.from_pretrained(model_name)
        self.model = BertForTokenClassification.from_pretrained(
            model_name,
            num_labels=num_labels,
            id2label=ID2LABEL,
            label2id=LABEL2ID
        )
        self.model.to(device)

    def train(self, train_dataset, val_dataset, epochs=3, batch_size=16, lr=2e-5):
        """训练模型"""

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size)

        optimizer = AdamW(self.model.parameters(), lr=lr)
        total_steps = len(train_loader) * epochs

        print("开始训练...")

        for epoch in range(epochs):
            self.model.train()
            total_loss = 0

            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}')

            for batch in progress_bar:
                optimizer.zero_grad()

                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                loss = outputs.loss
                total_loss += loss.item()

                loss.backward()
                optimizer.step()

                progress_bar.set_postfix({'loss': loss.item()})

            avg_loss = total_loss / len(train_loader)
            print(f'Epoch {epoch + 1} - 平均损失: {avg_loss:.4f}')

            # 验证
            val_metrics = self.evaluate(val_loader)
            print(f"验证集F1分数: {val_metrics['f1']:.4f}")

    def evaluate(self, dataloader):
        """评估模型"""
        self.model.eval()
        predictions = []
        true_labels = []

        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].cpu().numpy()

                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
                logits = outputs.logits

                batch_predictions = torch.argmax(logits, dim=2).cpu().numpy()

                # 移除padding和特殊token
                for i in range(len(batch_predictions)):
                    preds = []
                    truths = []

                    for j in range(len(batch_predictions[i])):
                        if labels[i][j] != -100:  # 忽略特殊token
                            preds.append(ID2LABEL[batch_predictions[i][j]])
                            truths.append(ID2LABEL[labels[i][j]])

                    predictions.append(preds)
                    true_labels.append(truths)

        # 计算指标
        f1 = f1_score(true_labels, predictions)

        return {
            'f1': f1,
            'predictions': predictions,
            'true_labels': true_labels
        }

    def predict(self, sentence):
        """预测单个句子"""
        self.model.eval()

        # 分词
        tokens = list(sentence)  # 对于中文，按字分割

        # 编码
        encoding = self.tokenizer(
            tokens,
            max_length=128,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
            is_split_into_words=True
        )

        with torch.no_grad():
            input_ids = encoding['input_ids'].to(device)
            attention_mask = encoding['attention_mask'].to(device)

            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            predictions = torch.argmax(logits, dim=2).cpu().numpy()[0]

            # 对齐预测结果
            aligned_predictions = []
            word_ids = encoding.word_ids()
            previous_word_idx = None

            for idx, word_idx in enumerate(word_ids):
                if word_idx is not None and word_idx != previous_word_idx:
                    label = ID2LABEL[predictions[idx]]
                    aligned_predictions.append((tokens[word_idx], label))
                previous_word_idx = word_idx

        return aligned_predictions

    def save_model(self, path):
        """保存模型"""
        self.model.save_pretrained(path)
        self.tokenizer.save_pretrained(path)
        print(f"模型已保存到: {path}")

    def load_model(self, path):
        """加载模型"""
        self.model = BertForTokenClassification.from_pretrained(path)
        self.tokenizer = BertTokenizer.from_pretrained(path)
        self.model.to(device)
        print(f"模型已从 {path} 加载")


def main():
    # 1. 准备数据
    print("准备数据...")
    sentences, labels = create_sample_data()

    # 分割训练集和验证集
    train_sentences, val_sentences, train_labels, val_labels = train_test_split(
        sentences, labels, test_size=0.25, random_state=SEED
    )

    # 2. 初始化tokenizer和数据集
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

    train_dataset = NERDataset(train_sentences, train_labels, tokenizer)
    val_dataset = NERDataset(val_sentences, val_labels, tokenizer)

    print(f"训练集大小: {len(train_dataset)}")
    print(f"验证集大小: {len(val_dataset)}")

    # 3. 训练模型
    print("\n初始化模型...")
    trainer = BERT_NER_Trainer()

    print("\n开始训练模型...")
    trainer.train(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        epochs=5,
        batch_size=2,
        lr=3e-5
    )

    # 4. 评估模型
    print("\n评估模型...")
    val_loader = DataLoader(val_dataset, batch_size=2)
    metrics = trainer.evaluate(val_loader)

    # 5. 保存模型
    trainer.save_model('./bert_ner_model')

    # 6. 示例预测
    print("\n示例预测:")
    test_sentences = [
        "马云在阿里巴巴工作",
        "清华大学位于北京市",
        "李娜赢得了法国网球公开赛冠军"
    ]

    for sentence in test_sentences:
        predictions = trainer.predict(sentence)
        print(f"\n句子: {sentence}")
        print("实体识别结果:")
        current_entity = []
        current_type = None

        for token, label in predictions:
            if label.startswith('B-'):
                if current_entity:
                    print(f"  {current_type}: {''.join(current_entity)}")
                current_entity = [token]
                current_type = label[2:]
            elif label.startswith('I-'):
                current_entity.append(token)
            else:
                if current_entity:
                    print(f"  {current_type}: {''.join(current_entity)}")
                    current_entity = []
                    current_type = None

        if current_entity:
            print(f"  {current_type}: {''.join(current_entity)}")


def load_and_predict():
    """加载已训练模型并进行预测"""

    # 初始化
    trainer = BERT_NER_Trainer()

    # 加载已训练模型
    trainer.load_model('./bert_ner_model')

    # 进行预测
    test_sentence = "苹果公司首席执行官蒂姆库克访问了中国上海"
    predictions = trainer.predict(test_sentence)

    print(f"测试句子: {test_sentence}")
    print("实体识别结果:")

    current_entity = []
    current_type = None

    for token, label in predictions:
        if label.startswith('B-'):
            if current_entity:
                print(f"  {current_type}: {''.join(current_entity)}")
            current_entity = [token]
            current_type = label[2:]
        elif label.startswith('I-'):
            current_entity.append(token)
        else:
            if current_entity:
                print(f"  {current_type}: {''.join(current_entity)}")
                current_entity = []
                current_type = None

    if current_entity:
        print(f"  {current_type}: {''.join(current_entity)}")


def batch_predict(sentences):
    """批量预测多个句子"""
    trainer = BERT_NER_Trainer()
    trainer.load_model('./bert_ner_model')

    results = []
    for sentence in sentences:
        predictions = trainer.predict(sentence)

        entities = {}
        current_entity = []
        current_type = None

        for token, label in predictions:
            if label.startswith('B-'):
                if current_entity:
                    entity_text = ''.join(current_entity)
                    if current_type not in entities:
                        entities[current_type] = []
                    if entity_text not in entities[current_type]:
                        entities[current_type].append(entity_text)
                current_entity = [token]
                current_type = label[2:]
            elif label.startswith('I-'):
                current_entity.append(token)
            else:
                if current_entity:
                    entity_text = ''.join(current_entity)
                    if current_type not in entities:
                        entities[current_type] = []
                    if entity_text not in entities[current_type]:
                        entities[current_type].append(entity_text)
                    current_entity = []
                    current_type = None

        if current_entity:
            entity_text = ''.join(current_entity)
            if current_type not in entities:
                entities[current_type] = []
            if entity_text not in entities[current_type]:
                entities[current_type].append(entity_text)

        results.append({
            'sentence': sentence,
            'entities': entities
        })

    return results


if __name__ == "__main__":
    # 训练模型
    main()

    # 或者直接加载已训练模型进行预测
    # load_and_predict()

    # 批量预测示例
    # test_sentences = ["比尔盖茨创建了微软公司", "北京大学是中国著名大学"]
    # results = batch_predict(test_sentences)
    # for result in results:
    #     print(f"\n句子: {result['sentence']}")
    #     print(f"实体: {result['entities']}")
